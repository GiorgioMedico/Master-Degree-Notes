\documentclass[openany]{book}

% Basic packages
\usepackage{amsmath, amsthm, graphicx, amsfonts, float, bm}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}

% Page geometry
\usepackage{geometry}
\geometry{
 a4paper,
 total={170mm,237mm},
 left=20mm,
 top=30mm,
}

% Hyperlinks and headers
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{tikz}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{\thepage}
\setlength{\footskip}{50pt}

% Image path
\graphicspath{ {./images/} }

% Custom commands and operators
\newcommand\at[2]{\left.#1\right|_{#2}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\des}{des}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\notimplies}{%
\mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\deriv}[1]{\displaystyle\frac{d}{d #1}}
\newcommand{\traj}{(\bar{\mathbf{x}},\bar{\mathbf{u}})}

% Theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}
\newtheorem*{corollary}{Corollary}

% Custom boxes for definitions and notes
\newcommand{\definitionbox}[1]{
\begin{tcolorbox}[colback=blue!5,colframe=blue!40!black,title=Definition]
 #1
\end{tcolorbox}
}
\newcommand{\note}[1]{
\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Note]
 #1
\end{tcolorbox}
}

\title{Quiz of the course Autonomous and Mobile Robotics}
\author{Giorgio Medico}
\date{fall semester 2024}

\begin{document}
\maketitle

\chapter{December 23, 2021 Exam}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item The configuration space of a unicycle mobile robot is:
    \begin{itemize}
        \item[\checkmark] $[x\; y\; \theta]^T \in \R^2 \times S$
        \item[] $[x\; y\; \theta]^T \in \R^3$
        \item[] $[x\; y\; \theta\; \gamma]^T \in \R^2 \times S^2$
    \end{itemize}

    \item A constraint is said non-holonomic if:
    \begin{itemize}
        \item[\checkmark] the differential relation between the coordinates is not reducible to finite form
        \item[] finite relations between the coordinates of the system are present
        \item[] if differentiable/integrable relations between the coordinates of the system are present
    \end{itemize}

    \item Given the constraints matrix equation in Pfaffian form $A(q)\dot{q} = 0$, the admissible robot speed:
    \begin{itemize}
        \item[\checkmark] is generated by a matrix $G(q)$ such that $\text{Im}(G(q)) = \text{Ker}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $\text{Ker}(G(q)) = \text{Im}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $G(q) = A(q)^{-1}, \forall q$
    \end{itemize}

    \item Consider an obstacle avoidance algorithm based on potential fields:
    \begin{itemize}
        \item[\checkmark] the overall potential is the sum of an attractive one (generated by the goal) and a repulsive one, generated by the obstacles
        \item[] a concave shape of the obstacle can in many cases avoid the problem of local minima
        \item[] the control consists in setting the velocity of the robot equal to the gradient of the potential
    \end{itemize}

    \item In Reinforcement Learning algorithms, the reward:
    \begin{itemize}
        \item[] must be a function of the agent state
        \item[\checkmark] can be a function of the environment state
        \item[] depends on time
    \end{itemize}

    \item In Reinforcement Learning algorithms, the agent:
    \begin{itemize}
        \item[\checkmark] selects actions to maximize total expected future reward
        \item[\checkmark] may require balancing immediate and long term rewards
        \item[] selects actions to minimize the task execution time
    \end{itemize}

    \item A process satisfies the Markov property if:
    \begin{itemize}
        \item[] the agent state is the same as the environment state
        \item[\checkmark] one can make predictions for the future of the process based solely on its present state
        \item[] one can make predictions for the future of the process only based on the process full history
    \end{itemize}

    \item In Reinforcement Learning, the policy:
    \begin{itemize}
        \item[] is the learning process that will maximize the sum of future rewards
        \item[] is a deterministic function of the state
        \item[\checkmark] the strategy that the agent employs to decide the action to take on the current state
    \end{itemize}

    \item The action value function is defined as:
    \begin{itemize}
        \item[] $q^\pi(s,a) = E^\pi[R_{t+1}|S_t = s, A_t = a]$
        \item[] $q^\pi(s,a) = E^\pi[G_t|S_t = s]$
        \item[\checkmark] $q^\pi(s,a) = E^\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s, A_t = a]$
    \end{itemize}

    \item The Bellman optimality equation for the state value function can be written as:
    \begin{itemize}
        \item[] $v^*(s) = \max v^\pi(s)$
        \item[\checkmark] $v^*(s) = \max_{a\in A} q^*(s,a)$
        \item[\checkmark] $v^*(s) = \max_{a\in A} E^*[G_t|S_t = s, A_t = a]$
    \end{itemize}
\end{enumerate}

\chapter{January 27, 2022 Exam}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item The configuration space of a bicycle mobile robot is:
    \begin{itemize}
        \item[] $[x\; y\; \theta]^T \in \R^2 \times S$
        \item[] $[x\; y\; \theta\; \gamma]^T \in \R^4$
        \item[\checkmark] $[x\; y\; \theta\; \gamma]^T \in \R^2 \times S^2$
    \end{itemize}

    \item A non-holonomic constraint:
    \begin{itemize}
        \item[\checkmark] cannot be fully integrated
        \item[] can be written in the configuration space
        \item[\checkmark] does not restrict the space of configurations but the instant robot mobility
    \end{itemize}

    \item In a trajectory following problem, the robot must asymptotically perform a desired trajectory:
    \begin{itemize}
        \item[\checkmark] that depends on a free parameter $s$
        \item[] that depends on time $t$
        \item[] that must be represented by a polynomial function
    \end{itemize}

    \item In map-based navigation, the robot:
    \begin{itemize}
        \item[\checkmark] plans the trajectory using a map of the environment
        \item[] must update the planned path on the based of sensor information
        \item[] navigates the environment on the base of the sensor information only
    \end{itemize}

    \item Sequential Monte-Carlo Localization:
    \begin{itemize}
        \item[\checkmark] is based on the simultaneous evaluation of multiple potential robot configurations called particles
        \item[\checkmark] resamples the particles on the based of the weights evaluated after prediction, innovation and normalization
        \item[] works only in case the robot configuration can be described by a Gaussian distribution
    \end{itemize}

    \item The Bellman optimality equation for the action value function can be written as:
    \begin{itemize}
        \item[] $q^*(s,a) = \max v^\pi(s)$
        \item[\checkmark] $q^*(s,a) = E^*[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s, A_t = a]$
        \item[] $q^*(s,a) = p(s', r|s,a)[r + \gamma \max_{a'\in A} q^*(s',a')]$
    \end{itemize}

    \item Monte-Carlo reinforcement learning:
    \begin{itemize}
        \item[] can be applied to non-episodic tasks
        \item[] requires the knowledge of the reward model
        \item[\checkmark] does not require the knowledge of the transition model
    \end{itemize}

    \item Under which hypotheses Monte-Carlo reinforcement learning methods converge to optimal value function?
    \begin{itemize}
        \item[\checkmark] infinite episodes
        \item[] deterministic policy
        \item[\checkmark] exploring starts
    \end{itemize}

    \item The $\lambda$-return is defined as:
    \begin{itemize}
        \item[\checkmark] $G^\lambda_t = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G^{(n)}_t$
        \item[] $G^\lambda_t = \sum_{k=0}^\infty \lambda^k R_{t+k+1}$
        \item[] $G^\lambda_t = R_{t+1} + \lambda V(S_{t+1})$
    \end{itemize}

    \item In value function approximation by stochastic gradient descent, the parameter vector update is defined as:
    \begin{itemize}
        \item[] $\Delta w = -\frac{1}{2}\alpha\nabla_w J(w)$
        \item[\checkmark] $\Delta w = \alpha(v^\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)$
        \item[] $\Delta w = \alpha E^\pi[(v^\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)]$
    \end{itemize}
\end{enumerate}


\chapter{June 14, 2022 Exam}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item A constraint is said non-holonomic if:
    \begin{itemize}
        \item[\checkmark] the differential relation between the coordinates is not reducible to finite form
        \item[] finite relations between the coordinates of the system are present
        \item[] if differentiable/integrable relations between the coordinates of the system are present
    \end{itemize}

    \item Consider a WMR with constraint matrix equation $A(q)\dot{q} = 0$, with $\dot{q} \in \R^N$:
    \begin{itemize}
        \item[] the equation can be fully integrated if the constraints represent N pure rolling wheels (no slipping)
        \item[\checkmark] the allowable speeds can be generated by a matrix $G(q)$ such that $\text{Im}(G(q)) = \text{Ker}(A(q))$
        \item[] the equation represents N non-holonomic constraints if slipping of the wheels is not allowed
    \end{itemize}

    \item The constraint introduced by a single wheel can be expressed as:
    \begin{itemize}
        \item[] $x\sin\theta - y\cos\theta = 0$
        \item[\checkmark] $\dot{x}\sin\theta - \dot{y}\cos\theta = 0$
        \item[] $x\sin\dot{\theta} - y\cos\dot{\theta} = 0$
    \end{itemize}

    \item Given the constraints matrix equation in Pfaffian form $A(q)\dot{q} = 0$, the admissible robot speed:
    \begin{itemize}
        \item[\checkmark] is generated by a matrix $G(q)$ such that $\text{Im}(G(q)) = \text{Ker}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $\text{Ker}(G(q)) = \text{Im}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $G(q) = A(q)^{-1}, \forall q$
    \end{itemize}

    \item In reactive navigation, the robot:
    \begin{itemize}
        \item[] plans the trajectory using a map of the environment
        \item[] updates the planned path on the based of sensor information
        \item[\checkmark] navigates the environment on the base of the sensor information only
    \end{itemize}

    \item Examples of map-based navigation algorithms are:
    \begin{itemize}
        \item[\checkmark] distance transform planning
        \item[\checkmark] $A^*$ and $D^*$
        \item[] bug algorithms
    \end{itemize}

    \item In map-based navigation, the robot:
    \begin{itemize}
        \item[\checkmark] plans the trajectory using a map of the environment
        \item[] updates the planned path on the based of sensor information
        \item[] navigates the environment on the base of the sensor information only
    \end{itemize}

    \item The state value function is defined as:
    \begin{itemize}
        \item[] $v^\pi(s) = E^\pi[R_{t+1}|S_t = s]$
        \item[\checkmark] $v^\pi(s) = E^\pi[G_t|S_t = s]$
        \item[] $v^\pi(s) = E^\pi[R_{t+1} + \gamma v^\pi(s')|S_t = s]$
    \end{itemize}

    \item The future discounted reward is defined as:
    \begin{itemize}
        \item[\checkmark] $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t+i+1}$
        \item[] $G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots = \sum_{i=0}^\infty R_{t+i+1}$
        \item[] $G_t = R_{t-1} + \gamma R_{t-2} + \gamma^2R_{t-3} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t-i-1}$
    \end{itemize}

    \item In Reinforcement Learning algorithms, the reward:
    \begin{itemize}
        \item[] must be a function of the agent state
        \item[\checkmark] can be a function of the environment state
        \item[] depends on time
    \end{itemize}
\end{enumerate}

\chapter{December 22, 2022 Exam}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item The configuration space of a unicycle mobile robot is:
    \begin{itemize}
        \item[] $[x\; y\; \theta]^T \in \R^3$
        \item[\checkmark] $[x\; y\; \theta]^T \in \R^2 \times S$
        \item[] $[x\; y\; \theta\; \gamma]^T \in \R^2 \times S^2$
    \end{itemize}

    \item A constraint is said non-holonomic if:
    \begin{itemize}
        \item[\checkmark] the differential relation between the coordinates is not reducible to finite form
        \item[] finite relations between the coordinates of the system are present
        \item[] if differentiable/integrable relations between the coordinates of the system are present
    \end{itemize}

    \item Given the constraints matrix equation in Pfaffian form $A(q)\dot{q} = 0$, the admissible robot speed:
    \begin{itemize}
        \item[] is generated by a matrix $G(q)$ such that $\text{Ker}(G(q)) = \text{Im}(A(q)), \forall q$
        \item[\checkmark] is generated by a matrix $G(q)$ such that $\text{Im}(G(q)) = \text{Ker}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $G(q) = A(q)^{-1}, \forall q$
    \end{itemize}

    \item Consider an obstacle avoidance algorithm based on potential fields:
    \begin{itemize}
        \item[] a concave shape of the obstacle can in many cases avoid the problem of local minima
        \item[] the control consists in setting the velocity of the robot equal to the gradient of the potential
        \item[\checkmark] the overall potential is the sum of an attractive potential generated by the goal and repulsive potentials generated by the obstacles
    \end{itemize}

    \item The robot configuration space is randomly sampled taking into account feasible trajectories:
    \begin{itemize}
        \item[] in the Probabilistic Roadmap algorithm
        \item[\checkmark] in the Rapidly-Exploring Random Tree
        \item[] in the Voronoi Roadmap algorithm
    \end{itemize}

    \item A process satisfies the Markov property if:
    \begin{itemize}
        \item[] the agent state is the same as the environment state
        \item[\checkmark] one can make predictions for the future of the process based solely on its present state
        \item[] one can make predictions for the future of the process only based on the process full history
    \end{itemize}

    \item The action value function is defined as:
    \begin{itemize}
        \item[] $q^\pi(s,a) = E^\pi[R_{t+1}|S_t = s, A_t = a]$
        \item[] $q^\pi(s,a) = E^\pi[G_t|S_t = s]$
        \item[\checkmark] $q^\pi(s,a) = E^\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s, A_t = a]$
    \end{itemize}

    \item The Bellman optimality equation for the state value function can be written as:
    \begin{itemize}
        \item[] $v^*(s) = \max v^\pi(s)$
        \item[\checkmark] $v^*(s) = \max_{a\in A} q^*(s,a)$
        \item[\checkmark] $v^*(s) = \max_{a\in A} E^*[G_t|S_t = s, A_t = a]$
    \end{itemize}

    \item The $\lambda$-return is defined as:
    \begin{itemize}
        \item[] $G^\lambda_t = R_{t+1} + \lambda V(S_{t+1})$
        \item[] $G^\lambda_t = \sum_{k=0}^\infty \lambda^k R_{t+k+1}$
        \item[\checkmark] $G^\lambda_t = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G^{(n)}_t$
    \end{itemize}

    \item In value function approximation by stochastic gradient descent, the parameter vector update is defined as:
    \begin{itemize}
        \item[\checkmark] $\Delta w = \alpha(v^\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)$
        \item[] $\Delta w = \alpha E^\pi[(v^\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)]$
        \item[] $\Delta w = -\frac{1}{2}\alpha\nabla_w J(w)$
    \end{itemize}
\end{enumerate}


\chapter{January 26, 2023 Exam}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item A non-holonomic constraint:
    \begin{itemize}
        \item[\checkmark] cannot be fully integrated
        \item[] can be written in the configuration space
        \item[\checkmark] does not restrict the space of configurations but the instant robot mobility
    \end{itemize}

    \item In a trajectory following problem, the robot must asymptotically perform a desired trajectory:
    \begin{itemize}
        \item[\checkmark] that depends on a free parameter $s$
        \item[] that depends on time $t$
        \item[] that must be represented by a polynomial function
    \end{itemize}

    \item In map-based navigation, the robot:
    \begin{itemize}
        \item[\checkmark] plans the trajectory using a map of the environment
        \item[] must update the planned path on the based of sensor information
        \item[] navigates the environment on the base of the sensor information only
    \end{itemize}

    \item Sequential Monte-Carlo Localization:
    \begin{itemize}
        \item[\checkmark] is based on the simultaneous evaluation of multiple potential robot configurations called particles
        \item[\checkmark] resamples the particles on the based of the weights evaluated after prediction, innovation and normalization
        \item[] works only in case the robot configuration can be described by a Gaussian distribution
    \end{itemize}

    \item The future discounted reward is defined as:
    \begin{itemize}
        \item[] $G_t = R_t + \gamma R_{t+1} + \gamma^2R_{t+2} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t+i}$
        \item[\checkmark] $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t+i+1}$
        \item[] $G_t = R_{t-1} + \gamma R_{t-2} + \gamma^2R_{t-3} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t-i-1}$
    \end{itemize}

    \item The Bellman optimality equation for the action value function can be written as:
    \begin{itemize}
        \item[] $q^*(s,a) = \max v^\pi(s)$
        \item[\checkmark] $q^*(s,a) = E^*[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s, A_t = a]$
        \item[] $q^*(s,a) = p(s', r|s,a)[r + \gamma \max_{a'\in A} q^*(s',a')]$
    \end{itemize}

    \item Monte-Carlo reinforcement learning:
    \begin{itemize}
        \item[] can be applied to non-episodic tasks
        \item[] requires the knowledge of the reward model
        \item[\checkmark] does not require the knowledge of the transition model
    \end{itemize}

    \item Under which hypotheses Monte-Carlo reinforcement learning methods converge to optimal value function?
    \begin{itemize}
        \item[\checkmark] infinite episodes
        \item[] deterministic policy
        \item[\checkmark] exploring starts
    \end{itemize}

    \item In backward view Sarsa($\lambda$), the eligibility trace update can be defined as:
    \begin{itemize}
        \item[] $E_t(s) = \gamma E_{t-1}(s) + \mathbf{1}(S_t = s)$
        \item[] $E_t(s) = \gamma\lambda E_{t-1}(s) + \mathbf{1}(S_t = s)$
        \item[\checkmark] $E_t(s,a) = \gamma\lambda E_{t-1}(s,a) + \mathbf{1}(S_t = s, A_t = a)$
    \end{itemize}

    \item TD($\lambda$) with Value Function Approximation is defined as:
    \begin{itemize}
        \item[] $\Delta w = \alpha(G_t - \hat{v}(S_t,w))\nabla_w \hat{v}(S_t,w)$
        \item[] $\Delta w = \alpha(R_{t+1} + \gamma\hat{v}(S_{t+1},w) - \hat{v}(S_t,w))\nabla_w \hat{v}(S_t,w)$
        \item[\checkmark] $\Delta w = \alpha(G^\lambda_t - \hat{v}(S_t,w))\nabla_w \hat{v}(S_t,w)$
    \end{itemize}
\end{enumerate}

\chapter{December 22, 2023 Exam}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item Given the constraints matrix equation in Pfaffian form $A(q)\dot{q} = 0$, the admissible robot speed:
    \begin{itemize}
        \item[\checkmark] is generated by a matrix $G(q)$ such that $\text{Im}(G(q)) = \text{Ker}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $\text{Ker}(G(q)) = \text{Im}(A(q)), \forall q$
        \item[] is generated by a matrix $G(q)$ such that $G(q) = A(q)^T, \forall q$
    \end{itemize}

    \item For a unicycle robot, given the geometric trajectory $x(s), y(s), \theta(s)$, it is possible to write the steering input $\omega(s)$ as:
    \begin{itemize}
        \item[] $\omega(s) = (\theta''(s)x'(s) - \theta''(s)y'(s))/(x'(s)^2 + y'(s)^2)$
        \item[\checkmark] $\omega(s) = (y''(s)x'(s) - x''(s)y'(s))/(x'(s)^2 + y'(s)^2)$
        \item[] $\omega(s) = (y''(s)\theta'(s) - x''(s)\theta'(s))/(x'(s)^2 - y'(s)^2)$
    \end{itemize}

    \item Consider Odometry for WMR:
    \begin{itemize}
        \item[\checkmark] it represents a reliable estimation of the robot position over a single evaluation step
        \item[] it presents an exact estimation for the x and y variables if the precise reconstruction method is used
        \item[] the precise reconstruction method is not affected by changes of the steering angle over a single step
    \end{itemize}

    \item Examples of map-based navigation algorithms are:
    \begin{itemize}
        \item[\checkmark] distance transform planning
        \item[\checkmark] $A^*$ and $D^*$
        \item[] bug algorithms
    \end{itemize}

    \item A process satisfies the Markov property if:
    \begin{itemize}
        \item[] the agent state is the same as the environment state
        \item[\checkmark] one can make predictions for the future of the process based solely on its present state
        \item[] one can make predictions for the future of the process only based on the process full history
    \end{itemize}

    \item The Bellman optimality equation for the state value function can be written as:
    \begin{itemize}
        \item[] $v^*(s) = \max v^\pi(s)$
        \item[\checkmark] $v^*(s) = \max_{a\in A} q^*(s,a)$
        \item[\checkmark] $v^*(s) = \max_{a\in A} E^*[G_t|S_t = s, A_t = a]$
    \end{itemize}

    \item The relative probability of the trajectory obtained following a target policy $\pi$ w.r.t. the behavior policy $\mu$ is:
    \begin{itemize}
        \item[] $\rho^T_t = \prod_{k=t}^{T-1} \pi(A_k|S_k)\prod_{k=t}^{T-1}\mu(A_k|S_k)$
        \item[\checkmark] $\rho^T_t = \prod_{k=t}^{T-1} \pi(A_k|S_k)/\prod_{k=t}^{T-1}\mu(A_k|S_k)$
        \item[] $\rho^T_t = \prod_{k=t}^{T-1} \mu(A_k|S_k)/\prod_{k=t}^{T-1}\pi(A_k|S_k)$
    \end{itemize}

    \item The $\lambda$-return is defined as:
    \begin{itemize}
        \item[] $G^\lambda_t = \sum_{k=0}^\infty \lambda^k R_{t+k+1}$
        \item[] $G^\lambda_t = R_{t+1} + \lambda V(S_{t+1})$
        \item[\checkmark] $G^\lambda_t = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G^{(n)}_t$
    \end{itemize}

    \item In value function approximation by stochastic gradient descent, the parameter vector update is defined as:
    \begin{itemize}
        \item[] $\Delta w = -\frac{1}{2}\alpha\nabla_w J(w)$
        \item[\checkmark] $\Delta w = \alpha(v^\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)$
        \item[] $\Delta w = \alpha E^\pi[(v^\pi(S) - \hat{v}(S,w))\nabla_w \hat{v}(S,w)]$
    \end{itemize}

    \item Given two tasks $a \in \mathbb{R}^{m_a}$ and $b \in \mathbb{R}^{m_b}$ which Jacobian matrices with respect to the robot configuration are $J_a$ and $J_b$, $a$ and $b$ are said:
    \begin{itemize}
        \item[\checkmark] orthogonal if $J_aJ_b^\# = 0^{m_a\times m_b}$, where $\#$ represents the matrix pseudoinverse
        \item[\checkmark] dependent if $\text{rank}(J_a^T) + \text{rank}(J_b^T) > \text{rank}([J_a^T\; J_b^T])$
        \item[] independent if $\text{rank}(J_a^T) + \text{rank}(J_b^T) < \text{rank}([J_a^T\; J_b^T])$
    \end{itemize}
\end{enumerate}

\chapter{Written Exam Sample}

\section*{Theory Questions}
Answer all questions. Some questions may have multiple correct answers.

\begin{enumerate}
    \item A constraint is said holonomic if:
    \begin{itemize}
        \item[] the differential relation between the coordinates is not reducible to finite form
        \item[\checkmark] finite relations between the coordinates of the system are present
        \item[\checkmark] if differentiable/integrable relations between the coordinates of the system are present
    \end{itemize}

    \item The constraint introduced by a single wheel can be expressed as:
    \begin{itemize}
        \item[] $x\sin\theta - y\cos\theta = 0$
        \item[\checkmark] $\dot{x}\sin\theta - \dot{y}\cos\theta = 0$
        \item[] $x\sin\dot{\theta} - y\cos\dot{\theta} = 0$
    \end{itemize}

    \item A single Swedish wheel:
    \begin{itemize}
        \item[] enables to control the motion along both the rolling and the driven directions
        \item[\checkmark] does not allow any control in the rolling direction
        \item[] does not allow to control the motion neither along the rolling nor the driven directions
    \end{itemize}

    \item In reactive navigation, the robot:
    \begin{itemize}
        \item[] plans the trajectory using a map of the environment
        \item[] updates the planned path on the based of sensor information
        \item[\checkmark] navigates the environment on the base of the sensor information only
    \end{itemize}

    \item The distance transform of a map:
    \begin{itemize}
        \item[\checkmark] has the same size of the original map
        \item[\checkmark] has elements which values is the distance to the target position
        \item[] can be computed only using Euclidean distance
    \end{itemize}

    \item Sequential Monte-Carlo Localization:
    \begin{itemize}
        \item[] resamples the particles on the based of their spatial distribution
        \item[] performs better than the EKF in case the robot configuration is described by a Gaussian distribution
        \item[\checkmark] works also in case the probability distribution function of the robot configuration is not known
    \end{itemize}

    \item In Reinforcement Learning algorithms, the reward:
    \begin{itemize}
        \item[\checkmark] is a scalar feedback signal
        \item[] is minimized by the agent
        \item[\checkmark] indicates how well agent is doing at step $t$
    \end{itemize}

    \item The future discounted reward is defined as:
    \begin{itemize}
        \item[\checkmark] $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t+i+1}$
        \item[] $G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots = \sum_{i=0}^\infty R_{t+i+1}$
        \item[] $G_t = R_{t-1} + \gamma R_{t-2} + \gamma^2R_{t-3} + \cdots = \sum_{i=0}^\infty \gamma^i R_{t-i-1}$
    \end{itemize}

    \item The agent state:
    \begin{itemize}
        \item[] is always the same as the environment state
        \item[\checkmark] is the same as the environment state in case of fully observable environments
        \item[\checkmark] is the same as the environment state in Markov decision processes
    \end{itemize}

    \item The state value function is defined as:
    \begin{itemize}
        \item[] $v^\pi(s) = E^\pi[R_{t+1}|S_t = s]$
        \item[\checkmark] $v^\pi(s) = E^\pi[G_t|S_t = s]$
        \item[\checkmark] $v^\pi(s) = E^\pi[R_{t+1} + \gamma v^\pi(s')|S_t = s]$
    \end{itemize}
\end{enumerate}


\section*{Open Questions}
Answer the following questions.

\begin{enumerate}
    \item Describe synthetically with words and formulas the main steps of the EKF localization:
    \begin{align*}
        \hat{q}^+\langle k+1\rangle &= f(\hat{q}\langle k\rangle, \hat{u}\langle k\rangle) &\text{State prediction}\\
        \hat{P}^+\langle k+1\rangle &= F_q\hat{P}\langle k\rangle F_q^T + F_v\hat{V}F_v^T &\text{Covariance projection}\\
        v &= z^\#\langle k+1\rangle - h(\hat{q}^+\langle k+1\rangle, p_i) &\text{Innovation}\\
        K &= \hat{P}^+\langle k+1\rangle H_q^T(H_q\hat{P}^+\langle k+1\rangle H_q^T + H_wWH_w^T)^{-1} &\text{Kalman gain}\\
        \hat{q}\langle k+1\rangle &= \hat{q}^+\langle k+1\rangle + Kv &\text{State update with innovation}\\
        \hat{P}\langle k+1\rangle &= \hat{P}^+\langle k+1\rangle - KH_q\hat{P}^+\langle k+1\rangle &\text{Covariance update}
    \end{align*}

    \item The $\lambda$-return $G^\lambda_t$ combining all n-step returns $G^{(n)}_t$ can be expressed as:
    \[G^\lambda_t = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G^{(n)}_t\]
\end{enumerate}

\end{document}

